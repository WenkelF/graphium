# @package _global_

# == Fine-tuning configs in Graphium ==
# 
# A fine-tuning config is a appendum to a (pre-)training config.
# Since many things (e.g. the architecture), will stay constant between (pre-)training and fine-tuning,
# this config should be as minimal as possible to avoid unnecessary duplication. It only specifies
# what to override with regards to the config used for (pre-)training.
# 
# Given the following training command: 
# >>> graphium-train --cfg /path/to/train.yaml
# 
# Fine-tuning now is as easy as: 
# >>> graphium-train --cfg /path/to/train.yaml +finetune=admet
#
# NOTE: This config can be used for each of the benchmarks in the TDC ADMET benchmark suite.
#     The only thing that needs to be changed is the `constants.task` key.


## == Overrides == 

defaults:
  # This file contains all metrics and loss function info for all ADMET tasks.
  # This config is filtered at runtime based on the `constants.task` key.
  - override /tasks/loss_metrics_datamodule: admet

constants:
  
  # For now, we assume a model is always fine-tuned on a single task at a time.
  # You can override this value with any of the benchmark names in the TDC benchmark suite.
  # See also https://tdcommons.ai/benchmark/admet_group/overview/
  task: bbb_martins

  name: finetuning_${constants.task}_large_mpnn
  wandb:
    name: ${constants.name}
    project: finetune_${constants.task}
    entity: wenkelf
  seed: 42
  max_epochs: 50
  data_dir: ../data/graphium/admet/${constants.task}
  raise_train_error: true

predictor:
  optim_kwargs:
    lr: 5e-5

# == Fine-tuning config == 

finetuning:

  # For now, we assume a model is always fine-tuned on a single task at a time.
  # You can override this value with any of the benchmark names in the TDC benchmark suite.
  # See also https://tdcommons.ai/benchmark/admet_group/overview/
  task: ${constants.task}
  level: graph

  # Pretrained model
  pretrained_model: large-mpnn-v2
  finetuning_module: task_heads  
  sub_module_from_pretrained: pcba_1328
  new_sub_module: ${constants.task} # optional
  
  # keep_modules_after_finetuning_module: # optional
  #   task_heads/pcqm4m_g25:
  #     new_sub_module: lipophilicity_astrazeneca
  #     out_dim: 1

  # Changes to finetuning_module                                                 
  drop_depth: 2
  new_out_dim: 1
  added_depth: 3
  new_last_activation: sigmoid

  # Training
  unfreeze_pretrained_depth: 0
  epoch_unfreeze_all: none

  # # Optional finetuning head appended to model after finetuning_module
  # finetuning_head:
  #   task: ${constants.task}
  #   previous_module: task_heads
  #   incoming_level: graph
  #   model_type: mlp
  #   in_dim: 8
  #   out_dim: 1
  #   hidden_dims: 8
  #   depth: 2
  #   last_layer_is_readout: true
