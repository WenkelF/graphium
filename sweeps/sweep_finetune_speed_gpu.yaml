program: graphium/cli/train_finetune_test.py
command:
  - ${env}
  - python
  - ${program}
  - accelerator=gpu
  - model=mpnn
  - architecture=largemix
  - tasks=largemix
  - training=largemix
  - predictor.torch_scheduler_kwargs=null
  - +datamodule.args.multiprocessing_context=tbd
  - datamodule.args.persistent_workers=false
  - +finetuning=admet_base
  - constants.max_epochs=2
  - constants.task=lipophilicity_astrazeneca
  - finetuning.keep_modules_after_finetuning_module.task_heads-pcqm4m_g25.last_activation=none
  - ${args_no_hyphens}
method: grid
metric:
  name: epoch_time
  goal: minimize
name: Benchmark processing time when finetuning on ADMET (gpu)
parameters:
  datamodule.args.batch_size_training:
    values:
      - 32
      - 128
      - 512
  datamodule.args.persistent_workers:
    values:
      - true
      - false
  datamodule.args.num_workers:
    values:
      # - 0
      - 1
      - 2
      - 4
  datamodule.args.multiprocessing_context:
    values:
      - fork
      - spawn
  finetuning.keep_modules_after_finetuning_module.task_heads-pcqm4m_g25.depth:
    value: 4
  finetuning.keep_modules_after_finetuning_module.task_heads-pcqm4m_g25.hidden_dims:
    value: 256

