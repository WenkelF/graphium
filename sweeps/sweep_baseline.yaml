program: graphium/cli/train_finetune_test.py
command:
  - ${env}
  - python
  - ${program}
  - accelerator=gpu
  - model=mpnn
  - architecture=largemix
  - tasks=largemix
  - training=largemix
  - predictor.torch_scheduler_kwargs=null
  - +finetuning=admet_baseline
  - constants.task=ames
  - ${args_no_hyphens}
method: grid
metric:
  name: loss/val
  goal: minimize
name: Baselines for ADMET -- ames -- different scales
parameters:
  constants.seed:
    values:
      - 100
      - 200
      - 300
  finetuning.pretrained_model:
    values:
      - 1M
      - 3M
      - 10M
      - 30M
      - 100M
  finetuning.epoch_unfreeze_all:
    values:
      - none
      - 0
      - 20
  finetuning.keep_modules_after_finetuning_module.task_heads-pcqm4m_g25.depth:
    values:
      - 1
      - 2

