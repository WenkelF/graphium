program: graphium/cli/train_finetune_test.py
command:
  - ${env}
  - python
  - ${program}
  - accelerator=gpu
  - model=mpnn
  - architecture=largemix
  - tasks=largemix
  - training=largemix
  - predictor.torch_scheduler_kwargs=null
  - +finetuning=admet_scales
  - constants.task=bbb_martins
  - finetuning.keep_modules_after_finetuning_module.task_heads-pcqm4m_g25.depth=3
  - finetuning.keep_modules_after_finetuning_module.task_heads-pcqm4m_g25.hidden_dims=256
  - finetuning.keep_modules_after_finetuning_module.task_heads-pcqm4m_g25.last_activation=sigmoid
  - datamodule.args.batch_size_training=256
  - predictor.optim_kwargs.lr=0.000005
  - finetuning.epoch_unfreeze_all=0
  - trainer.early_stopping.monitor=graph_bbb_martins/auroc/val
  - trainer.early_stopping.mode=max
  - ${args_no_hyphens}
method: grid
metric:
  name: graph_bbb_martins/auroc/val
  goal: maximize
name: Finetune on ADMET -- bbb_martins -- differnt scales
parameters:
  finetuning.pretrained_model:
    values:
      - 1M
      - 3M
      - 10M
      - 30M
      - 100M
  constants.seed:
    values:
      - 100
      - 200
      - 300
  # finetuning.unfreeze_pretrained_depth:
  #   distribution: int_uniform
  #   max: 2
  #   min: 0
  # finetuning.epoch_unfreeze_all:
  #   distribution: int_uniform
  #   max: 100
  #   min: 0

