program: graphium/cli/train_finetune_test.py
command:
  - ${env}
  - python
  - ${program}
  - accelerator=cpu
  - model=mpnn
  - architecture=largemix
  - tasks=largemix
  - training=largemix
  # - predictor.torch_scheduler_kwargs=null
  - +finetuning=admet_base
  - constants.task=bbb_martins
  - finetuning.pretrained_model=large-mpnn-10M
  - finetuning.keep_modules_after_finetuning_module.task_heads-pcqm4m_g25.last_activation=sigmoid
  - ${args_no_hyphens}
method: bayes
metric:
  name: loss/val
  goal: minimize
name: Finetune on ADMET -- bbb_martins -- 10M
parameters:
  datamodule.args.batch_size_training:
    distribution: categorical
    values:
      - 16
      - 32
      - 64
      - 128
      - 256
      - 512
  predictor.optim_kwargs.lr:
    distribution: log_uniform_values
    max: 1e-04
    min: 1e-07
  finetuning.keep_modules_after_finetuning_module.task_heads-pcqm4m_g25.depth:
    distribution: int_uniform
    max: 4
    min: 1
  finetuning.keep_modules_after_finetuning_module.task_heads-pcqm4m_g25.hidden_dims:
    distribution: int_uniform
    max: 512
    min: 64
  # finetuning.unfreeze_pretrained_depth:
  #   distribution: int_uniform
  #   max: 2
  #   min: 0
  # finetuning.epoch_unfreeze_all:
  #   distribution: int_uniform
  #   max: 100
  #   min: 0

